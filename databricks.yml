# This is a Databricks asset bundle definition for B3.
# See https://docs.databricks.com/dev-tools/bundles/index.html for documentation.
bundle:
  name: B3

include:
  - resources/*.yml

resources:
  jobs:
    b3_job:
      name: b3_job

      schedule:
        quartz_cron_expression: '44 37 8 * * ?'
        timezone_id: Europe/Amsterdam

      email_notifications:
        on_failure:
          - rodrigo.santos@blueshift.com.br

      tasks:
        - task_key: notebook_t
          job_cluster_key: job_cluster
          notebook_task:
            notebook_path: 1.bronze/up2data/exploration.ipynb
          # libraries:
          #   - file:
          #     path: utils/requirements.txt

          #   - pypi: 
          #         package: "numpy==1.25.2"
          #         repo: utils/requirements.txt

        - task_key: notebook_task
          job_cluster_key: job_cluster
          notebook_task:
            notebook_path: 1.bronze/sql/exploration.ipynb
            # notebook_path: ../1.bronze/exploration.ipynb

        # - task_key: refresh_pipeline
        #   pipeline_task:
        #     pipeline_id: ${resources.pipelines.b3_pipeline.id}

        - task_key: main_task
          job_cluster_key: job_cluster
          depends_on:
            - task_key: notebook_task
            - task_key: notebook_t

          notebook_task:
            notebook_path: 2.silver/sql/exploration.ipynb
        
      job_clusters:
        - job_cluster_key: job_cluster
          new_cluster:
            spark_version: 13.3.x-scala2.12
            node_type_id: Standard_D3_v2
            autoscale:
                min_workers: 1
                max_workers: 1



targets:
  # The 'dev' target, used for development purposes.
  # Whenever a developer deploys using 'dev', they get their own copy.
  dev:
    # We use 'mode: development' to make sure everything deployed to this target gets a prefix
    # like '[dev my_user_name]'. Setting this mode also disables any schedules and
    # automatic triggers for jobs and enables the 'development' mode for Delta Live Tables pipelines.
    mode: development
    default: true
    workspace:
      host: https://adb-4292359788598238.18.azuredatabricks.net
      root_path: /Shared/${workspace.current_user.userName}/dev
      #root_path: /Shared/dev/

  # Optionally, there could be a 'staging' target here.
  # (See Databricks docs on CI/CD at https://docs.databricks.com/dev-tools/bundles/index.html.)
  #
  # staging:
  #  workspace:
  #    host: https://adb-4292359788598238.18.azuredatabricks.net

  # The 'prod' target, used for production deployment.
  prod:
    # For production deployments, we only have a single copy, so we override the
    # workspace.root_path default of
    # /Users/${workspace.current_user.userName}/.bundle/${bundle.target}/${bundle.name}
    # to a path that is not specific to the current user.
    mode: production
    workspace:
      host: https://adb-4292359788598238.18.azuredatabricks.net
      root_path: /Shared/.bundle/prod
      # root_path: /Sha~red/.bundle/prod/${bundle.name}
    run_as:
      # This runs as rodrigo.santos@blueshift.com.br in production. Alternatively,
      # a service principal could be used here using service_principal_name
      # (see Databricks documentation).
      user_name: ${workspace.current_user.userName}


   